# Explorando Capas Convolucionales a TravÃ©s de Datos y Experimentos

**Autor**: Deisy Lorena GuzmÃ¡n Cabrales 

---

## Tabla de Contenidos
- [DescripciÃ³n del Problema](#descripciÃ³n-del-problema)
- [DescripciÃ³n del Dataset](#descripciÃ³n-del-dataset)
- [DiseÃ±o de Arquitectura](#diseÃ±o-de-arquitectura)
- [Resultados Experimentales](#resultados-experimentales)
- [InterpretaciÃ³n y Conocimientos Clave](#interpretaciÃ³n-y-conocimientos-clave)
- [Estructura del Repositorio](#estructura-del-repositorio)
- [ConfiguraciÃ³n y Requisitos](#configuraciÃ³n-y-requisitos)
- [CÃ³mo Ejecutar](#cÃ³mo-ejecutar)
- [Despliegue del Modelo](#despliegue-del-modelo)
- [Referencias](#referencias)

---

## DescripciÃ³n del Problema

Este proyecto explora las **redes neuronales convolucionales (CNNs)** no como modelos de caja negra, sino como componentes arquitectÃ³nicos cuyas decisiones de diseÃ±o afectan directamente el rendimiento, escalabilidad e interpretabilidad.

### Objetivos de Aprendizaje

1. Comprender la intuiciÃ³n matemÃ¡tica detrÃ¡s de las capas convolucionales
2. Analizar cÃ³mo las decisiones arquitectÃ³nicas (tamaÃ±o de kernel, profundidad, stride, padding) afectan el aprendizaje
3. Comparar capas convolucionales con capas totalmente conectadas para datos tipo imagen
4. Realizar anÃ¡lisis exploratorio de datos (EDA) significativo para tareas de redes neuronales
5. Comunicar decisiones arquitectÃ³nicas y experimentales de manera clara

### Enfoque

En lugar de seguir una receta, este proyecto:
- **Selecciona** un dataset apropiado con justificaciÃ³n
- **Analiza** caracterÃ­sticas del dataset a travÃ©s de EDA
- **DiseÃ±a** arquitecturas CNN desde cero con razonamiento explÃ­cito
- **Conduce** experimentos controlados sobre parÃ¡metros arquitectÃ³nicos
- **Interpreta** resultados a travÃ©s del lente del sesgo inductivo
- **Despliega** el modelo para inferencia en producciÃ³n

---

## DescripciÃ³n del Dataset

### Fashion-MNIST

**Fuente**: [TensorFlow Keras Datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist)  
**Repositorio Original**: https://github.com/zalandoresearch/fashion-mnist

### CaracterÃ­sticas del Dataset

- **Tarea**: ClasificaciÃ³n de imÃ¡genes multi-clase
- **Clases**: 10 categorÃ­as de moda
  - 0: Camiseta/top
  - 1: PantalÃ³n
  - 2: SuÃ©ter
  - 3: Vestido
  - 4: Abrigo
  - 5: Sandalia
  - 6: Camisa
  - 7: Zapatilla deportiva
  - 8: Bolso
  - 9: BotÃ­n

- **TamaÃ±o**: 
  - Conjunto de entrenamiento: 60,000 imÃ¡genes
  - Conjunto de prueba: 10,000 imÃ¡genes
- **Dimensiones de imagen**: 28Ã—28 pÃ­xeles
- **Canales**: 1 (escala de grises)
- **Rango de pÃ­xeles**: [0, 255]
- **Balance de clases**: Perfectamente balanceado (6,000 muestras por clase en entrenamiento)

### Â¿Por QuÃ© Fashion-MNIST?

Fashion-MNIST es ideal para estudiar capas convolucionales porque:

1. **Estructura Espacial**: Las prendas de moda contienen patrones locales (texturas, bordes) que se benefician de la conectividad local
2. **Invariancia por TraslaciÃ³n**: Los objetos permanecen reconocibles independientemente de su posiciÃ³n â€“ exactamente lo que proporciona la convoluciÃ³n
3. **CaracterÃ­sticas JerÃ¡rquicas**: Las capas inferiores detectan bordes/texturas, las capas mÃ¡s profundas detectan partes de objetos (mangas, tacones)
4. **Desafiante pero Manejable**: MÃ¡s difÃ­cil que los dÃ­gitos MNIST, haciendo visibles las diferencias arquitectÃ³nicas
5. **TamaÃ±o PrÃ¡ctico**: Cabe en memoria y entrena rÃ¡pidamente para experimentaciÃ³n rÃ¡pida

---

## DiseÃ±o de Arquitectura

### Modelo Base (No Convolucional)

**PropÃ³sito**: Establecer referencia de rendimiento sin sesgo inductivo espacial

```
Input (784) 
    â†“
Dense(128, ReLU) + Dropout(0.2)
    â†“
Dense(64, ReLU) + Dropout(0.2)
    â†“
Dense(10, Softmax)
```

**ParÃ¡metros**: ~101,000  
**LimitaciÃ³n Clave**: Trata los pÃ­xeles como caracterÃ­sticas independientes, ignorando la estructura espacial

---

### Modelo CNN (Convolucional)

**Arquitectura Propuesta**:

```
Input (28Ã—28Ã—1)
    â†“
Conv2D(32 filtros, 3Ã—3, ReLU) + BatchNorm
    â†“
Conv2D(32 filtros, 3Ã—3, ReLU) + BatchNorm
    â†“
MaxPooling(2Ã—2)
    â†“
Conv2D(64 filtros, 3Ã—3, ReLU) + BatchNorm
    â†“
Conv2D(64 filtros, 3Ã—3, ReLU) + BatchNorm
    â†“
MaxPooling(2Ã—2)
    â†“
Flatten
    â†“
Dense(128, ReLU) + Dropout(0.3)
    â†“
Dense(10, Softmax)
```

**ParÃ¡metros**: ~94,000  
**Ventaja**: Menos parÃ¡metros que el modelo base, mejor rendimiento

---

### Justificaciones ArquitectÃ³nicas

#### Â¿Por QuÃ© Kernels 3Ã—3?
- **Eficiente**: Dos capas 3Ã—3 = mismo campo receptivo que 5Ã—5, pero menos parÃ¡metros
- **MÃ¡s no-linealidad**: Apilar capas pequeÃ±as agrega funciones de activaciÃ³n
- **EstÃ¡ndar de la industria**: Usado en VGG, ResNet y arquitecturas modernas

#### Â¿Por QuÃ© Dos Capas Conv Antes del Pooling?
- **CaracterÃ­sticas jerÃ¡rquicas**: La primera capa encuentra bordes, la segunda los combina
- **Preserva resoluciÃ³n**: No reduce el muestreo demasiado pronto
- **Agrega profundidad**: MÃ¡s poder representacional

#### Â¿Por QuÃ© MaxPooling 2Ã—2?
- **Reduce gradualmente**: Dimensiones espaciales 28â†’14â†’7
- **Invariancia por traslaciÃ³n**: PequeÃ±os desplazamientos no afectan la salida
- **Reduce parÃ¡metros**: Hace que las capas mÃ¡s profundas sean computacionalmente factibles
- **RegularizaciÃ³n**: Ayuda a prevenir el sobreajuste

#### Â¿Por QuÃ© Aumentar Filtros (32â†’64)?
- **Compensar pÃ©rdida espacial**: A medida que las dimensiones se reducen, agregar mÃ¡s canales
- **Capturar complejidad**: Las capas mÃ¡s profundas necesitan caracterÃ­sticas mÃ¡s abstractas
- **PrÃ¡ctica estÃ¡ndar**: ComÃºn en arquitecturas CNN

#### Â¿Por QuÃ© BatchNormalization?
- **Estabiliza el entrenamiento**: Normaliza las entradas de las capas
- **Convergencia mÃ¡s rÃ¡pida**: Permite tasas de aprendizaje mÃ¡s altas
- **RegularizaciÃ³n**: PrevenciÃ³n leve del sobreajuste

---

## Resultados Experimentales

### Modelo Base vs CNN

| Modelo | PrecisiÃ³n de Prueba | PÃ©rdida de Prueba | ParÃ¡metros | Tiempo de Entrenamiento |
|--------|---------------------|-------------------|------------|-------------------------|
| **Base (Dense)** | ~87-88% | ~0.35 | 101,000 | ~30s |
| **CNN (3Ã—3)** | ~91-92% | ~0.25 | 94,000 | ~45s |
| **Mejora** | **+4-5%** | **-0.10** | **-7%** | +15s |

**ConclusiÃ³n Clave**: CNN logra mejor precisiÃ³n con menos parÃ¡metros al explotar la estructura espacial.

---

### Experimento Controlado: TamaÃ±o de Kernel

**Pregunta de InvestigaciÃ³n**: Â¿CÃ³mo afecta el tamaÃ±o del kernel el rendimiento del modelo?

**Variables de control**: NÃºmero de capas, filtros, pooling, hiperparÃ¡metros de entrenamiento

| TamaÃ±o de Kernel | PrecisiÃ³n de Prueba | ParÃ¡metros | Tiempo de Entrenamiento | Observaciones |
|------------------|---------------------|------------|-------------------------|---------------|
| **3Ã—3** | ~91.5% | 94,000 | 45s | Mejor balance entre precisiÃ³n y eficiencia |
| **5Ã—5** | ~91.3% | 250,000 | 65s | MÃ¡s parÃ¡metros, precisiÃ³n similar |
| **7Ã—7** | ~89-90% | 450,000 | 85s | Demasiado agresivo para imÃ¡genes 28Ã—28 |

**ConclusiÃ³n**: Los kernels 3Ã—3 son Ã³ptimos para Fashion-MNIST â€“ kernels mÃ¡s grandes no mejoran la precisiÃ³n pero aumentan la complejidad.

---

### Visualizaciones

El notebook incluye:
- AnÃ¡lisis de distribuciÃ³n de clases
- ImÃ¡genes de muestra por clase
- Distribuciones de intensidad de pÃ­xeles
- Curvas de entrenamiento/validaciÃ³n
- GrÃ¡ficos de comparaciÃ³n de tamaÃ±o de kernel
- AnÃ¡lisis de compensaciones de rendimiento

---

## InterpretaciÃ³n y Conocimientos Clave

### Por QuÃ© las CNNs Superan al Modelo Base

1. **Conectividad Local**: Aprovecha la correlaciÃ³n entre pÃ­xeles cercanos (bordes, texturas)
2. **ComparticiÃ³n de ParÃ¡metros**: El mismo filtro aplicado en todas partes â†’ invariancia por traslaciÃ³n
3. **Aprendizaje JerÃ¡rquico**: AbstracciÃ³n progresiva desde bordes hasta partes hasta objetos
4. **AlineaciÃ³n del Sesgo Inductivo**: Las suposiciones de CNN coinciden con la estructura de imagen

### Â¿QuÃ© Sesgo Inductivo Introduce la ConvoluciÃ³n?

**Tres sesgos clave:**

1. **Localidad**: Los pÃ­xeles cercanos son mÃ¡s relevantes que los pÃ­xeles distantes
2. **Equivariancia por TraslaciÃ³n**: Los patrones son significativos independientemente de su posiciÃ³n
3. **ComposiciÃ³n JerÃ¡rquica**: Los patrones complejos se construyen a partir de patrones mÃ¡s simples

Estos sesgos:
- Reducen el espacio de hipÃ³tesis â†’ aprendizaje mÃ¡s rÃ¡pido
- Requieren menos datos â†’ mejor generalizaciÃ³n
- Codifican conocimiento del dominio â†’ rendimiento mejorado

### Â¿CuÃ¡ndo NO SerÃ­a Apropiada la ConvoluciÃ³n?

La convoluciÃ³n es **inapropiada** para:

1. **Datos Tabulares/Estructurados**: Sin estructura espacial (edad, ingresos, etc.)
2. **Tareas Sensibles a la PosiciÃ³n**: La ubicaciÃ³n importa (diagnÃ³stico en imagen mÃ©dica)
3. **Dependencias de Largo Alcance**: Patrones alejados en el espacio
4. **Grafos Irregulares**: Redes sociales, molÃ©culas (necesita GNNs)
5. **Datos Secuenciales con Orden Variable**: Algunas tareas de NLP (usar Transformers)
6. **Datasets Muy PequeÃ±os**: Datos insuficientes para aprender filtros

**Conocimiento Clave**: Las elecciones arquitectÃ³nicas codifican suposiciones â€“ las CNNs tienen Ã©xito cuando las suposiciones coinciden con la estructura del problema.

---

## Estructura del Repositorio

```
Exploring-Convolutional-Layers-Through-Data-and-Experiments/
â”‚
â”œâ”€â”€ README.md                                    # DocumentaciÃ³n del proyecto
â”œâ”€â”€ requirements.txt                             # Dependencias de Python (PyTorch)
â”œâ”€â”€ .gitignore                                   # Reglas de ignorar de Git
â”œâ”€â”€ convolutional_layers_workshop.ipynb          # Notebook completo
â”‚
â””â”€â”€ fashion_mnist_cnn_model_pytorch/             # Artefactos del modelo entrenado 
```

---

## ðŸ› ï¸ ConfiguraciÃ³n y Requisitos

### Prerrequisitos

- Python 3.9+ (Â¡Compatible con Python 3.14!)
- PyTorch 2.0+
- Jupyter Notebook o JupyterLab

### InstalaciÃ³n

1. **Clonar el repositorio**:
   ```bash
   git clone https://github.com/tuusuario/Exploring-Convolutional-Layers-Through-Data-and-Experiments.git
   cd Exploring-Convolutional-Layers-Through-Data-and-Experiments
   ```

2. **Crear entorno virtual** (recomendado):
   ```bash
   python -m venv venv
   source venv/bin/activate  # En Windows: venv\Scripts\activate
   ```

3. **Instalar dependencias**:
   ```bash
   pip install torch torchvision numpy matplotlib seaborn pandas jupyter
   ```

   O usar requirements.txt:
   ```bash
   pip install -r requirements.txt
   ```

### Requirements.txt

```
torch>=2.0.0
torchvision>=0.15.0
numpy>=1.23.0
matplotlib>=3.5.0
seaborn>=0.12.0
pandas>=1.5.0
jupyter>=1.0.0
onnx>=1.14.0  # Para exportaciÃ³n de modelos
```

---

## CÃ³mo Ejecutar

### OpciÃ³n 1: Jupyter Notebook

1. Iniciar Jupyter:
   ```bash
   jupyter notebook
   ```

2. Abrir `convolutional_layers_workshop.ipynb`

3. Ejecutar todas las celdas secuencialmente (Cell â†’ Run All)

### OpciÃ³n 2: JupyterLab

1. Iniciar JupyterLab:
   ```bash
   jupyter lab
   ```

2. Abrir `convolutional_layers_workshop.ipynb`

3. Ejecutar celdas en orden


El notebook:
- DescargarÃ¡ Fashion-MNIST automÃ¡ticamente (solo en la primera ejecuciÃ³n)
- GenerarÃ¡ visualizaciones en lÃ­nea
- MostrarÃ¡ el progreso de entrenamiento y mÃ©tricas
- GuardarÃ¡ el modelo entrenado en `fashion_mnist_cnn_model_pytorch/`
- UsarÃ¡ GPU automÃ¡ticamente si estÃ¡ disponible (CUDA)

---



## Conclusiones Clave

1. **Las redes neuronales no son cajas negras** â€“ las elecciones arquitectÃ³nicas importan
2. **El sesgo inductivo es una caracterÃ­stica, no un error** â€“ codifica conocimiento del dominio
3. **La experimentaciÃ³n supera la intuiciÃ³n** â€“ prueba las suposiciones sistemÃ¡ticamente
4. **La simplicidad a menudo gana** â€“ los kernels 3Ã—3 superan alternativas mÃ¡s grandes
5. **ComprensiÃ³n > PrecisiÃ³n** â€“ saber POR QUÃ‰ funciona tu modelo
